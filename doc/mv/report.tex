\documentclass{acmtog}

\acmVolume{VV}
\acmNumber{N}
\acmYear{2012}
\acmMonth{February}
\acmArticleNum{XXX}  
\acmdoi{10.1145/XXXXXXX.YYYYYYY}

\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{relsize}

\begin{document}

\markboth{A.C. Macedo | M. Palhas | P. Costa}{Integrated Project}

\title{Integrated Project: Vision controlled camera} % title

\author{
Ana Catarina Macedo --- {\ttfamily a54773@alunos.uminho.pt}\\
Miguel Palhas --- {\ttfamily pg19808@alunos.uminho.pt}\\
Pedro Costa --- {\ttfamily pg19830@alunos.uminho.pt}\\
	\affil{University of Minho\\Department of Informatics}
}

\category{I.5.4}{Pattern recognition}{Applications}[Computer Vision]
\category{I.3.7}{Computer Graphics}{Three-Dimensional Graphics and Realism}[Animation]

\terms{Experimentation}

\keywords{camera control}

\maketitle

\begin{abstract}
\end{abstract}


\section{Introduction}
In this document is presented the second stage of the Integrated Project module of the Computer Graphics Specialization Unit of the Master's Degree in Informatics Engineering.

In the previous stage, the \texttt{ARToolKit} library was used to identify a pattern in the image captured by a usual webcam. Using the same library, one could also retrieve the required transformation matrix to position an object in the same position of the pattern, thus achieving a basic augmented reality system.

For that stage, the goal was to extract the translation and rotation data from the transformation matrix, so the model could be manually positioned in the pattern.

The translation data did not present any obstacle, as it is very straight forward to extract.

The rotation data though, consisted in extracting the Euler angles from the $3 \times 3$ top-left submatrix. This was hard because of three things:
\begin{itemize}
\item{Rotations and scales both take effect in this $3 \times 3$ submatrix. The problem was simplified by ignoring scales, as no method to remove them was found;}
\item{Even assuming that every rotation is performed only around the coordinate system axes (to simplify), the final result depends on the order by which the rotations are applied. The \texttt{ARToolKit} library does not mention this in the documentation. This order was assumed to be the standard Heading-Pitch-Roll (Y-X-Z);}
\item{Given the angles, the rotation matrix values are generated with trigonometric functions. The inverse functions return a smaller limited ranges of values for the angles, which results in a very unstable positioning of the model in the pattern.}
\end{itemize}
Despite existing a function the \texttt{ARToolKit} API to retrieve the angles, this function was not documented. The angles retrieved from this function showed the same stability problems of the inverse trigonometric approach.

For this stage, given a camera in a virtual 3-D world, the goal is to capture the movement of the pattern and use it to change the camera's perspective accordingly.

\section{Implementation}
The implemented system consists on two distinct parts: the capture system, controlled by the \texttt{ARToolKit} library, and the virtual world where the camera to be controlled exists.

The capture system searches for the pattern, and then updates a matrix shared\footnote{The concurrent accesses are controlled using \textit{mutexes} from the \texttt{Boost} library.} by both parts. If a pattern is found, the transformation matrix is retrieved from the library and saved in the shared container. Otherwise, it is set to be the identity matrix. This sets the direction of the camera to the default when no pattern is found by the camera, effectively resetting the perspective to the initial state. An alternative would be to leave the matrix untouched in this case, leaving the camera in the same direction as was defined by the last found  pattern.

The camera starts at the world coordinate system origin. It also has a direction vector (negative in the $z$ axis, by default), which defines the point to look at and an ``up'' direction (positive in the $y$ axis, by default). The direction vector allows the camera to follow the angles of the pattern both horizontally and vertically. The ``up'' vector, on the other hand, allows the camera to follow the rotation of the pattern in its own plane.

To achieve the rotation of the camera, at the beginning of each frame the normalized transformation matrix $N$ it computed as the inverse of the transpose of the original transformation matrix $T$, to remove the influence of scales. The normalized transformation matrix is then multiplied by the direction vector ($d$) and the up vector ($u$), generating the computed versions ($d'$ and $u'$, respectively).\footnote{The default values are used in the multiplication, otherwise the pattern would act like a steering wheel in a continuous camera movement. This would imply adding much complexity to the solution to  allow some control over the velocity, and would most probably bring back the difficulties found in the previous stage with the Euler angles.}. Any translation of $\Delta s$ space units to be performed since the computation of the last frame is applied to the position vector $p$ along the computed direction $d'$. This same vector $d'$ is then used to define the target position $t$ at a distance of $z$ space units from the camera. These computed values are the ones passed to the \texttt{lookAt} function. 

\begin{IEEEeqnarray*}{rCl}
N & = & ( T^{T} )^{-1}	\\
d' & = & N \times d 	\\
u' & = & N \times u 	\\
p & = & p + d' * \Delta s \\
t  & = & p + d' * z		
\end{IEEEeqnarray*}

\vfill

\subsection{Problems}

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=0.6\columnwidth]{images/coordsys.jpg}
	\end{center}
	\caption{The \texttt{ARToolKit} coordinate systems for the camera and the identified pattern.}
	\label{fig:coordsys}
\end{figure}

\autoref{fig:coordsys} shows how the \texttt{ARToolKit} library ``sees'' coordinate systems for both the capture device and the detected pattern. Two problems arise from the differences between these.

First, one can easily note that the vertical axes ($y$) have opposing directions. This causes the camera to be the correct way up when the pattern is in fact upside-down, and vice-versa. This can be easily corrected by using the opposite of the computed up vector. However, this is not required, since the camera easily follows the required rotations for the user to correct the perspective.

Second, the $z$ axes are also opposing, and the impact of this is even greater. It causes the camera to rotate 180 degrees when the pattern is recognized, and because of that the camera ends up ``looking'' backwards\footnote{This problem can be seen in \autoref{fig:screenshots}. The blue teapots are facing the opposite way in the second image, because the camera is in fact facing backwards.}. This can be easily solved by adapting the identity matrix returned when no pattern is detected to invert the $z$ component of the computed direction. While this does not prevent the camera from being rotated, it preserves this behaviour even when no pattern is detected, creating a visually acceptable solution.

\subsection{Testing}

The camera control is tested in a scene with various teapots in a cube shape around the origin of the world coordinate system. To draw the teapots, different shaders are used to change the colors. Each teapot receives a color related to its position in the cube:
\begin{enumerate}
\item{\textbf{Red} for the teapots in the faces intersected by the $x$ axis;}
\item{\textbf{Green} for those in the faces intersected by the $y$ axis;}
\item{\textbf{Blue} for the ones in the faces intersected by the $z$ axis.}
\end{enumerate}
The teapots in the edges receive the color from both faces they belong to, and the ones in the corners are drawn in white. All the shaders use Phong's algorithm lit from a point in space. The light and placement of the teapots help in recognizing the camera's movement, while the color system allows for some orientation while testing, so the viewer is able to distinguish the various possible movements.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[width=\columnwidth]{images/screenshot/default.png}
		\includegraphics[width=\columnwidth]{images/screenshot/ftl.png}
	\end{center}
	\caption[Application screenshots]{Screenshots of the working application. On top, the default view (camera looking in the negative $z$ direction); below, the view with the pattern pointing to the frontal top left corner of the cube (negative $x$, positive $y$, negative $z$).}
	\label{fig:screenshots}
\end{figure}

\section{Conclusion}
This document presented the implementation of a 3-D world scene with a vision controlled perspective. The implemented system uses the \texttt{ARToolKit} library to recognize the pattern and applies its orientation to the virtual camera, successfully allowing the viewer to ``look around'' in the 3-D world. A basic navigation system was also implemented, where the movement depends on the current direction of the camera.

Before reaching the current stage, two distinct solutions were prepared to answer this stage of the Integrated Project, but each implemented a system different than the one intended. The reason for this to happen was the lack of a guiding example, or a text describing the assignment. Although the simplicity of the project allowed to keep correcting, the misinterpretations which arised from this resulted in a heavy time consumption preparing code and studying approaches which will be ignored in any evaluation process.

\end{document}
